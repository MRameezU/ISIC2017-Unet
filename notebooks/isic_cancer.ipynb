{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"},"colab":{"provenance":[],"include_colab_link":true},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"widgets":{"application/vnd.jupyter.widget-state+json":{"790f89c0f5d3446bb93c625ab50bd7b6":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_628f6ddf22e0440684f4bf1b8c97f091","IPY_MODEL_f1a524add1d842a9b5baa2efb0e4895f","IPY_MODEL_79a3442b0c9545cd8ea80a3d0792f5d3"],"layout":"IPY_MODEL_3c6b6113be284a83b0b9fb09d7ce56a5"}},"628f6ddf22e0440684f4bf1b8c97f091":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3836531ebb994b97b76c8146cad100c3","placeholder":"​","style":"IPY_MODEL_0613ec503f5549fc9d401d1e36c957d7","value":"Downloading ISIC-2017_Training_Data.zip: 100%"}},"f1a524add1d842a9b5baa2efb0e4895f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e6165ffd55b245eda552096b62b74d76","max":6229496702,"min":0,"orientation":"horizontal","style":"IPY_MODEL_77c81513bdc94575aa3e8c2eb6016a80","value":6229496702}},"79a3442b0c9545cd8ea80a3d0792f5d3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0254039fe4e44b8485ed79f99452e85f","placeholder":"​","style":"IPY_MODEL_20ef24f8eef84039a901bbe2117032bf","value":" 5.80G/5.80G [02:15&lt;00:00, 51.3MB/s]"}},"3c6b6113be284a83b0b9fb09d7ce56a5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3836531ebb994b97b76c8146cad100c3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0613ec503f5549fc9d401d1e36c957d7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e6165ffd55b245eda552096b62b74d76":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"77c81513bdc94575aa3e8c2eb6016a80":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"0254039fe4e44b8485ed79f99452e85f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"20ef24f8eef84039a901bbe2117032bf":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"92b2f18bf9804fdb8e6f70d6ccc3abdc":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_8e1699f4ac5248c4b1d1d2b5cde41bc6","IPY_MODEL_c08b0f7377af49508484849d630b7201","IPY_MODEL_7182260f3ab14335a48e432738724c51"],"layout":"IPY_MODEL_754ac99334c24e4e9c43906c0269a476"}},"8e1699f4ac5248c4b1d1d2b5cde41bc6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4ed60d443a244d9c960a2b6fd073794a","placeholder":"​","style":"IPY_MODEL_c91678b18be64313a7b890b6cf58dc68","value":"Downloading ISIC-2017_Training_Part1_GroundTruth.zip: 100%"}},"c08b0f7377af49508484849d630b7201":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_aa484277cb1f41968c710c9b8e109b06","max":9321981,"min":0,"orientation":"horizontal","style":"IPY_MODEL_0e9d00b1c2ee4f7bbe447658b44f82ee","value":9321981}},"7182260f3ab14335a48e432738724c51":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a11b2ef901e2477793e3c172a0f90393","placeholder":"​","style":"IPY_MODEL_49ec8f535ff349c7ba518b1097d27d9c","value":" 8.89M/8.89M [00:00&lt;00:00, 16.8MB/s]"}},"754ac99334c24e4e9c43906c0269a476":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4ed60d443a244d9c960a2b6fd073794a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c91678b18be64313a7b890b6cf58dc68":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"aa484277cb1f41968c710c9b8e109b06":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0e9d00b1c2ee4f7bbe447658b44f82ee":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a11b2ef901e2477793e3c172a0f90393":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"49ec8f535ff349c7ba518b1097d27d9c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f1e9479a7eeb4ed7bea481e062e1f348":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_09631f462aab42ab858bf988f26fd2b5","IPY_MODEL_99c556633562454da093028106ac7acc","IPY_MODEL_9e182331b1a94ddcbf48f3700b7fee08"],"layout":"IPY_MODEL_29dbc36111c2426092ee7dda6f191891"}},"09631f462aab42ab858bf988f26fd2b5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_432d36f0d1474cfebf16520058711bc8","placeholder":"​","style":"IPY_MODEL_7b2b10fafe244ca39e20244458e34c0b","value":"Downloading ISIC-2017_Validation_Data.zip: 100%"}},"99c556633562454da093028106ac7acc":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_5415beafa7274df09eaaebee66d81bd0","max":920895119,"min":0,"orientation":"horizontal","style":"IPY_MODEL_0bde2dcd636c49e88d3cca548e196e17","value":920895119}},"9e182331b1a94ddcbf48f3700b7fee08":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_60bcba4eaa6148849054e5ba4e61174f","placeholder":"​","style":"IPY_MODEL_27e533353afd4a448801d8c1c97a7012","value":" 878M/878M [00:28&lt;00:00, 59.6MB/s]"}},"29dbc36111c2426092ee7dda6f191891":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"432d36f0d1474cfebf16520058711bc8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7b2b10fafe244ca39e20244458e34c0b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5415beafa7274df09eaaebee66d81bd0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0bde2dcd636c49e88d3cca548e196e17":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"60bcba4eaa6148849054e5ba4e61174f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"27e533353afd4a448801d8c1c97a7012":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cb713e9334b0411f8f807ef4da336db8":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6aea51f140ec4eba9413134f02e7d2b1","IPY_MODEL_44208c4de78441e69577d766d98aded8","IPY_MODEL_d1637a49303140d7a4616df478411db3"],"layout":"IPY_MODEL_19325b994ee64e05ace13d79cbcb2109"}},"6aea51f140ec4eba9413134f02e7d2b1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4099d28adfee47299cfb25083ad7492e","placeholder":"​","style":"IPY_MODEL_4b1b9b9d5ca045ada18fc8d105cc562f","value":"Downloading ISIC-2017_Training_Part1_GroundTruth.zip: 100%"}},"44208c4de78441e69577d766d98aded8":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_425e04f7a5a948f0a5fd03492d74959b","max":572320,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ab25df3228ad44268e5cdda5660b2f24","value":572320}},"d1637a49303140d7a4616df478411db3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3e4cdda76f4b44f5bebc3eecf9fecf7e","placeholder":"​","style":"IPY_MODEL_2e0351f5fb8c471090aafbfa07747791","value":" 559k/559k [00:00&lt;00:00, 1.72MB/s]"}},"19325b994ee64e05ace13d79cbcb2109":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4099d28adfee47299cfb25083ad7492e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4b1b9b9d5ca045ada18fc8d105cc562f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"425e04f7a5a948f0a5fd03492d74959b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ab25df3228ad44268e5cdda5660b2f24":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"3e4cdda76f4b44f5bebc3eecf9fecf7e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2e0351f5fb8c471090aafbfa07747791":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a1f7b037106f4cd894797c281a7a67c3":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_0b27c0a6f41d479a8929cd880c4e5283","IPY_MODEL_3adc53c2ad144dd88c2511213c2e1304","IPY_MODEL_aa2ef4da2dd148ddb57f6a0b0813f843"],"layout":"IPY_MODEL_d8a4f33518a54f999aa0fe0b8fd8836e"}},"0b27c0a6f41d479a8929cd880c4e5283":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_99c9d8d12b684c1aba639626fd48c056","placeholder":"​","style":"IPY_MODEL_2ea628b7bbe7452c8e12245e08f9fde8","value":"  0%"}},"3adc53c2ad144dd88c2511213c2e1304":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_0bc8c9791086402aad5b1ee84a3d8c4c","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f9b9f38e42d849cd8cb9cda4e7502bd7","value":0}},"aa2ef4da2dd148ddb57f6a0b0813f843":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a55637dbc65742c6844b668eb7b174ae","placeholder":"​","style":"IPY_MODEL_ee829598969a4b3891f2b94dd5d9f748","value":" 0/2 [00:00&lt;?, ?it/s]"}},"d8a4f33518a54f999aa0fe0b8fd8836e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"99c9d8d12b684c1aba639626fd48c056":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2ea628b7bbe7452c8e12245e08f9fde8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0bc8c9791086402aad5b1ee84a3d8c4c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f9b9f38e42d849cd8cb9cda4e7502bd7":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a55637dbc65742c6844b668eb7b174ae":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ee829598969a4b3891f2b94dd5d9f748":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<a href=\"https://colab.research.google.com/github/MRameezU/ISIC2017-Unet/blob/main/notebooks/isic_cancer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>","metadata":{"id":"view-in-github","colab_type":"text"}},{"cell_type":"code","source":"!nvidia-smi","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5q621Tar8oaF","outputId":"28758b36-ef4e-453a-c02f-369c40d407d2","trusted":true,"execution":{"iopub.status.busy":"2024-11-28T10:48:38.055899Z","iopub.execute_input":"2024-11-28T10:48:38.056991Z","iopub.status.idle":"2024-11-28T10:48:39.154909Z","shell.execute_reply.started":"2024-11-28T10:48:38.056939Z","shell.execute_reply":"2024-11-28T10:48:39.154058Z"}},"outputs":[{"name":"stdout","text":"Thu Nov 28 10:48:38 2024       \n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n| N/A   42C    P8              9W /   70W |       1MiB /  15360MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n|   1  Tesla T4                       Off |   00000000:00:05.0 Off |                    0 |\n| N/A   42C    P8             10W /   70W |       1MiB /  15360MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n                                                                                         \n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n|  No running processes found                                                             |\n+-----------------------------------------------------------------------------------------+\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"## 0- Get Setup","metadata":{"id":"cqpposRHAZ2v"}},{"cell_type":"code","source":"# !pip install --upgrade torch\n# !pip install --upgrade torchvision","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T10:48:39.156633Z","iopub.execute_input":"2024-11-28T10:48:39.156963Z","iopub.status.idle":"2024-11-28T10:48:39.160768Z","shell.execute_reply.started":"2024-11-28T10:48:39.156934Z","shell.execute_reply":"2024-11-28T10:48:39.159989Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"import torch\nimport torchvision\n\nprint(f\"torch version: {torch.__version__}\")\nprint(f\"torchvision version: {torchvision.__version__}\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3QrZrTI0AYos","outputId":"c1f2d9bb-83b2-4d56-f231-58f93bc85b2d","trusted":true,"execution":{"iopub.status.busy":"2024-11-28T10:48:39.161935Z","iopub.execute_input":"2024-11-28T10:48:39.162244Z","iopub.status.idle":"2024-11-28T10:48:39.169355Z","shell.execute_reply.started":"2024-11-28T10:48:39.162208Z","shell.execute_reply":"2024-11-28T10:48:39.168561Z"}},"outputs":[{"name":"stdout","text":"torch version: 2.4.0\ntorchvision version: 0.19.0\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"device agnostic code","metadata":{"id":"mKyWSzwRA870"}},{"cell_type":"code","source":"device= \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndevice","metadata":{"id":"hlW2JJNeA_3C","colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"32c76739-7b47-4666-a969-cc3dc53bf910","trusted":true,"execution":{"iopub.status.busy":"2024-11-28T10:48:39.170496Z","iopub.execute_input":"2024-11-28T10:48:39.170783Z","iopub.status.idle":"2024-11-28T10:48:39.233683Z","shell.execute_reply.started":"2024-11-28T10:48:39.170746Z","shell.execute_reply":"2024-11-28T10:48:39.232911Z"}},"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"'cuda'"},"metadata":{}}],"execution_count":9},{"cell_type":"markdown","source":"## 1 - Gettting Data","metadata":{"id":"NAC6C5n8BNL9"}},{"cell_type":"markdown","source":"### 1.0 - Downloading and Reorganizing training data","metadata":{"id":"r7EF3r-5UuPr"}},{"cell_type":"code","source":"import requests\n\nimport zipfile\n\nfrom pathlib import Path\n\nfrom tqdm.notebook import tqdm\n\n# training data\n\n# path to data folder\n\ndata_path = Path(\"data/\")\ntrain_data_path = data_path / \"train\"\nbinary_mask_data_path = data_path / \"binary\"\ntrain_zip_url = \"https://isic-challenge-data.s3.amazonaws.com/2017/ISIC-2017_Training_Data.zip\"\ntrain_zip_file = data_path / \"ISIC-2017_Training_Data.zip\"\n\n# binary mask\n\ntrain_binary_zip_url = \"https://isic-challenge-data.s3.amazonaws.com/2017/ISIC-2017_Training_Part1_GroundTruth.zip\"\ntrain_binary_zip_file = data_path / \"ISIC-2017_Training_Part1_GroundTruth.zip\"\n\n\ndef download_file(url, dest_path):\n    \"\"\"Downloads a file from a URL to a destination path with progress bar.\"\"\"\n    response = requests.get(url, stream=True)\n    response.raise_for_status()  # Raise an error for bad responses\n    total_size = int(response.headers.get('content-length', 0))\n    with open(dest_path, \"wb\") as file, tqdm(\n        desc=f\"Downloading {dest_path.name}\",total=total_size,\n        unit=\"B\",unit_scale=True,unit_divisor=1024) as bar:\n        for chunk in response.iter_content(chunk_size=1024):\n            file.write(chunk)\n            bar.update(len(chunk))\n    print(f\"Download Complete: {dest_path}\")\n\n\n\ndef extract_zip(file_path, extract_to):\n    \"\"\"Extracts a zip file to the specified directory.\"\"\"\n    with zipfile.ZipFile(file_path, mode=\"r\") as zip_file:\n        print(f\"Extracting {file_path.name} to {extract_to}\")\n        zip_file.extractall(extract_to)\n    print(f\"Extraction Complete: {extract_to}\")","metadata":{"id":"DrgctOIJGAYu","trusted":true,"execution":{"iopub.status.busy":"2024-11-28T10:48:39.235732Z","iopub.execute_input":"2024-11-28T10:48:39.236064Z","iopub.status.idle":"2024-11-28T10:48:39.370698Z","shell.execute_reply.started":"2024-11-28T10:48:39.236014Z","shell.execute_reply":"2024-11-28T10:48:39.370125Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# Main script\n\nif train_data_path.is_dir() and binary_mask_data_path.is_dir():\n    print(f\"{train_data_path} and {binary_mask_data_path} directories already exist.\")\nelse:\n    print(f\"Preparing data directories at {data_path}\")\n    train_data_path.mkdir(parents=True, exist_ok=True)\n\n    # Download training data\n\n    print(f\"Downloading Training Data from: {train_zip_url}\")\n    download_file(train_zip_url, train_zip_file)\n\n    # Extract the zip file\n    extract_zip(train_zip_file, train_data_path)\n    binary_mask_data_path.mkdir(parents=True,exist_ok=True)\n    # Download training data\n    print(f\"Downloading Binary Mask Data from: {train_binary_zip_url}\")\n    download_file(train_binary_zip_url, train_binary_zip_file)\n    # Extract the zip file\n    extract_zip(train_binary_zip_file, binary_mask_data_path)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":237,"referenced_widgets":["790f89c0f5d3446bb93c625ab50bd7b6","628f6ddf22e0440684f4bf1b8c97f091","f1a524add1d842a9b5baa2efb0e4895f","79a3442b0c9545cd8ea80a3d0792f5d3","3c6b6113be284a83b0b9fb09d7ce56a5","3836531ebb994b97b76c8146cad100c3","0613ec503f5549fc9d401d1e36c957d7","e6165ffd55b245eda552096b62b74d76","77c81513bdc94575aa3e8c2eb6016a80","0254039fe4e44b8485ed79f99452e85f","20ef24f8eef84039a901bbe2117032bf","92b2f18bf9804fdb8e6f70d6ccc3abdc","8e1699f4ac5248c4b1d1d2b5cde41bc6","c08b0f7377af49508484849d630b7201","7182260f3ab14335a48e432738724c51","754ac99334c24e4e9c43906c0269a476","4ed60d443a244d9c960a2b6fd073794a","c91678b18be64313a7b890b6cf58dc68","aa484277cb1f41968c710c9b8e109b06","0e9d00b1c2ee4f7bbe447658b44f82ee","a11b2ef901e2477793e3c172a0f90393","49ec8f535ff349c7ba518b1097d27d9c"]},"id":"wKpRiks2GAYv","outputId":"e000f9c9-a08e-403d-b6c3-dcc2be8d6772","trusted":true,"execution":{"iopub.status.busy":"2024-11-28T10:48:39.371522Z","iopub.execute_input":"2024-11-28T10:48:39.371749Z"}},"outputs":[{"name":"stdout","text":"Preparing data directories at data\nDownloading Training Data from: https://isic-challenge-data.s3.amazonaws.com/2017/ISIC-2017_Training_Data.zip\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading ISIC-2017_Training_Data.zip:   0%|          | 0.00/5.80G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b4013cb12e064dccb60d259c3a001334"}},"metadata":{}}],"execution_count":null},{"cell_type":"markdown","source":"the data contain the Images and their respective Superpixel mask and we have to download Binary mask seperately\n#### Seperating the Inputs and Ouputs\nour train data folder contain both Training Images and SuperPixel mask therfore seperating them into different folders","metadata":{"id":"QVCs8M53GQY5"}},{"cell_type":"code","source":"import os\nfrom pathlib import Path\nimport shutil\n\ndef organize_files(dataset_folder, image_output_folder, superpixel_output_folder):\n    \"\"\"\n    Organize files by separating images and superpixel masks into different folders.\n\n    Args:\n        dataset_folder (str or Path): Path to the folder containing both images and masks.\n        image_output_folder (str or Path): Path to the folder where images will be moved.\n        superpixel_output_folder (str or Path): Path to the folder where superpixel masks will be moved\n    \"\"\"\n    # Convert paths to Path objects\n    dataset_folder = Path(dataset_folder)\n    image_output_folder = Path(image_output_folder)\n    superpixel_output_folder = Path(superpixel_output_folder)\n\n    # Create output folders if they don't exist\n    image_output_folder.mkdir(parents=True, exist_ok=True)\n    superpixel_output_folder.mkdir(parents=True, exist_ok=True)\n    # Iterate through all files in the dataset folder\n    for file in dataset_folder.iterdir():\n        if file.is_file():\n            if file.name.endswith(\".jpg\"):\n                # Move image file\n                shutil.move(str(file), str(image_output_folder / file.name))\n            elif file.name.endswith(\"_superpixels.png\"):\n                # Move superpixel mask file\n                shutil.move(str(file), str(superpixel_output_folder / file.name))\n    print(f\"Files have been organized. Images moved to {image_output_folder}, masks to {superpixel_output_folder}.\")\n\nif __name__ == \"__main__\":\n\n    dataset_folder=train_data_path / \"ISIC-2017_Training_Data\"   #\"data/train/ISIC-2017_Data\"\n    image_output_folder=\"ISIC-2017_Data/train/Images\"\n    superpixel_output_folder=\"ISIC-2017_Data/train/Superpixel\"\n\n    organize_files(dataset_folder=dataset_folder,\n                   image_output_folder=image_output_folder,\n                   superpixel_output_folder=superpixel_output_folder)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iABOH3KcGmu0","outputId":"e55b8ffd-b1f2-4aba-d6e2-abd5ba5cdd80","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Moving Binary masks","metadata":{"id":"jZe4lYgoIjTb"}},{"cell_type":"code","source":"import shutil\n\n# moving file to a consolidated location\nsource_dir = binary_mask_data_path / \"ISIC-2017_Training_Part1_GroundTruth\" #Path(\"data/train/ISIC-2017_Training_Part1_GroundTruth\")\ndestination_dir = Path(\"ISIC-2017_Data/train/Binary\")\n# Create the destination directory if it doesn't exist\ndestination_dir.mkdir(parents=True, exist_ok=True)\nfor file in source_dir.iterdir():\n  if file.is_file():\n    shutil.move(str(file),str(destination_dir/file.name))","metadata":{"id":"DFV5Y5mxKZD6","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Deleting the Extras\n\nDeleting the `data_path` folder to save storage","metadata":{"id":"_YKNO7ODdu-w"}},{"cell_type":"code","source":"# Deleting our data_path after getting our desizerd ouput to free storage\nif data_path.exists() and data_path.is_dir():\n    shutil.rmtree(data_path)\n    print(f\"Folder '{data_path}' and all its subdirectories have been deleted.\")\nelse:\n    print(f\"Folder '{data_path}' does not exist.\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"v5lyaxEYVRoL","outputId":"3d04d518-7a17-4385-a955-20e117363e21","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Result\n\n ISIC-2017_Data This folder contains the dataset for the ISIC 2017 skin cancer segmentation project. The dataset is organized into the following subfolders:\n\n ## Structure\n\n\n\n### Subfolders\n\n\n\n1. **Images/**\n\n   - Contains the original images used for training and validation.\n\n\n\n2. **Superpixel/**\n\n   - Contains the superpixel masks generated for each image.\n\n\n\n3. **Binary/**\n\n   - Contains the binary masks indicating the regions of interest in each image.\n","metadata":{"id":"-T9Liv6oGAYy"}},{"cell_type":"markdown","source":"### 1.1 - Downloading reorganizing Validation data","metadata":{"id":"f9ZoDRT6QNYD"}},{"cell_type":"code","source":"# path to data folder\ndata_path = Path(\"data/\")\nvalidaiton_data_path = data_path / \"valid\"\nbinary_mask_data_path = data_path / \"binary\"\nvalid_zip_url = \"https://isic-challenge-data.s3.amazonaws.com/2017/ISIC-2017_Validation_Data.zip\"\nvalid_zip_file = data_path / \"ISIC-2017_Validation_Data.zip\"\n# binary mask\nvalid_binary_zip_url = \"https://isic-challenge-data.s3.amazonaws.com/2017/ISIC-2017_Validation_Part1_GroundTruth.zip\"\nvalid_binary_zip_file = data_path / \"ISIC-2017_Training_Part1_GroundTruth.zip\"\n","metadata":{"id":"FiKofLxSVD3c","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if validaiton_data_path.is_dir() and binary_mask_data_path.is_dir():\n    print(f\"{validaiton_data_path} and {binary_mask_data_path} directories already exist.\")\n\nelse:\n    print(f\"Preparing data directories at {data_path}\")\n    validaiton_data_path.mkdir(parents=True, exist_ok=True)\n\n    # Download training data\n    print(f\"Downloading validation Data from: {valid_zip_url}\")\n    download_file(valid_zip_url, valid_zip_file)\n\n    # Extract the zip file\n    extract_zip(valid_zip_file, validaiton_data_path)\n    binary_mask_data_path.mkdir(parents=True,exist_ok=True)\n\n    # Download training data\n    print(f\"Downloading Binary Mask Data from: {train_binary_zip_url}\")\n    download_file(valid_binary_zip_url, valid_binary_zip_file)\n\n    # Extract the zip file\n    extract_zip(valid_binary_zip_file, binary_mask_data_path)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":237,"referenced_widgets":["f1e9479a7eeb4ed7bea481e062e1f348","09631f462aab42ab858bf988f26fd2b5","99c556633562454da093028106ac7acc","9e182331b1a94ddcbf48f3700b7fee08","29dbc36111c2426092ee7dda6f191891","432d36f0d1474cfebf16520058711bc8","7b2b10fafe244ca39e20244458e34c0b","5415beafa7274df09eaaebee66d81bd0","0bde2dcd636c49e88d3cca548e196e17","60bcba4eaa6148849054e5ba4e61174f","27e533353afd4a448801d8c1c97a7012","cb713e9334b0411f8f807ef4da336db8","6aea51f140ec4eba9413134f02e7d2b1","44208c4de78441e69577d766d98aded8","d1637a49303140d7a4616df478411db3","19325b994ee64e05ace13d79cbcb2109","4099d28adfee47299cfb25083ad7492e","4b1b9b9d5ca045ada18fc8d105cc562f","425e04f7a5a948f0a5fd03492d74959b","ab25df3228ad44268e5cdda5660b2f24","3e4cdda76f4b44f5bebc3eecf9fecf7e","2e0351f5fb8c471090aafbfa07747791"]},"id":"4zmQVG2WfxOY","outputId":"afd8d0e9-10cc-4fd5-e3ac-c00ea92e6c66","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dataset_folder=validaiton_data_path / \"ISIC-2017_Validation_Data\"   #\"/content/data/valid/ISIC-2017_Validation_Data\"\nimage_output_folder=\"ISIC-2017_Data/valid/Images\"\nsuperpixel_output_folder=\"ISIC-2017_Data/valid/Superpixel\"\norganize_files(dataset_folder=dataset_folder,\n\n                   image_output_folder=image_output_folder,\n\n                   superpixel_output_folder=superpixel_output_folder)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7gCZEp10g-V_","outputId":"80aaa592-cdf1-44dc-8d85-84c977c77352","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import shutil\n\n# moving file to a consolidated location\nsource_dir = binary_mask_data_path / \"ISIC-2017_Validation_Part1_GroundTruth\" #Path(\"/content/data/binary/ISIC-2017_Validation_Part1_GroundTruth\")\ndestination_dir = Path(\"ISIC-2017_Data/valid/Binary\")\n\n# Create the destination directory if it doesn't exist\ndestination_dir.mkdir(parents=True, exist_ok=True)\nfor file in source_dir.iterdir():\n  if file.is_file():\n    shutil.move(str(file),str(destination_dir/file.name))","metadata":{"id":"_-hTly_Mhv5T","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Deleting our data_path after getting our desizerd ouput to free storage\nif data_path.exists() and data_path.is_dir():\n    shutil.rmtree(data_path)\n    print(f\"Folder '{data_path}' and all its subdirectories have been deleted.\")\nelse:\n    print(f\"Folder '{data_path}' does not exist.\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nIJxe-aTiUvD","outputId":"41e7e9f2-3988-476e-8575-0a5aa97f73ab","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 2 - Become one with the data (data preparation and exploration)","metadata":{"id":"QhFwyiB0h5WX"}},{"cell_type":"code","source":"import os\n\ndef walk_through(dir_path:str):\n  for dirpath, dirnames, filenames in os.walk(dir_path):\n    print(f\"There are {len(dirnames)} directories and {len(filenames)} images in '{dirpath}'.\")","metadata":{"id":"BjJghcgqh-aD","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data_path=Path(\"ISIC-2017_Data\")\nwalk_through(data_path)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e5I0LWzFiAmw","outputId":"46ca07ba-6962-41b1-a3ca-af68bc65a01b","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 2.1 Visualize Images\n\nLet's write some code to:\n\n\n\n1. Get all of the image paths using `pathlib.Path.glob()` to find all of the files ending in .jpg.\n\n2. Pick a random image path using Python's `random.choice()`.\n\n3. And since we're working with images, we'll open the random image path using `PIL.Image.open()` (PIL stands for Python Image Library).\n","metadata":{"id":"-gTqd6KYif3X"}},{"cell_type":"code","source":"import random\nfrom PIL import Image\n\n# creating a list of all the images\nimage_path_list=list(data_path.glob(\"**/*.jpg\"))\nlen(image_path_list)\n\n# select a random image path\nrandom_image_path = random.choice(image_path_list)\n\n# open Image\n\nimg = Image.open(fp=random_image_path)\nprint(f\"Random Image Path: {random_image_path}\")\nprint(f\"Image height: {img.height}\")\nprint(f\"Image width: {img.width}\")\n\n# Display the image\nimg","metadata":{"id":"6NmCDxeUjdsg","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"random_images_path_idx=random.sample(population=range(len(image_path_list)),k=3)\nrandom_images_path_idx","metadata":{"id":"BIJrb-pglHvB","colab":{"base_uri":"https://localhost:8080/"},"outputId":"1373c02e-5497-43da-fcca-7747f95869fd","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# list for inut images\nimage_path_list=sorted(list(data_path.glob(\"**/*.jpg\")))\n\n# list for binary masks\nbinary_mask_list=sorted(list(data_path.glob(\"**/Binary/*.png\")))\n\n# list for superpixel mask\nsuperpixel_mask_list=sorted(list(data_path.glob(\"**/Superpixel/*.png\")))\nlen(binary_mask_list),len(superpixel_mask_list)","metadata":{"id":"Xmbz1UViqG2N","colab":{"base_uri":"https://localhost:8080/"},"outputId":"13d8f91b-5475-4964-d08a-8ab04b90e347","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import random\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nfrom typing import List\n\ndef plot_random_images(\n    image_list: List[str],\n    binary_mask_list: List[str],\n    superpixel_mask_list: List[str],\n    num_samples: int = 3):\n\n    \"\"\"\n    Plots a specified number of random images alongside their binary and superpixel masks.\n\n    Args:\n        image_list (List[str]): List of file paths to input images.\n        binary_mask_list (List[str]): List of file paths to binary mask images.\n        superpixel_mask_list (List[str]): List of file paths to superpixel mask images.\n        num_samples (int, optional): Number of random samples to display. Defaults to 3.\n    Raises:\n        ValueError: If the lengths of `image_list`, `binary_mask_list`, and `superpixel_mask_list` are not equal.\n        ValueError: If `num_samples` is greater than the number of available images.\n\n    Example:\n        ```python\n        plot_random_images(\n            image_list=[\"image1.jpg\", \"image2.jpg\"],\n            binary_mask_list=[\"binary1.png\", \"binary2.png\"],\n            superpixel_mask_list=[\"superpixel1.png\", \"superpixel2.png\"],\n            num_samples=2)\n        ```\n    \"\"\"\n    # Check for consistency in list lengths\n    if len(image_list) != len(binary_mask_list) or len(image_list) != len(superpixel_mask_list):\n        raise ValueError(\"All input lists must have the same length.\")\n    # Ensure the number of samples is valid\n    if num_samples > len(image_list):\n        raise ValueError(\"num_samples cannot be greater than the number of images available.\")\n    # Randomly sample indices\n    idx = random.sample(range(len(image_list)), num_samples)\n    # Create subplots\n    fig, axes = plt.subplots(nrows=num_samples, ncols=3, figsize=(15, 5 * num_samples))\n    # Ensure axes is a list of lists, even for single sample\n    if num_samples == 1:\n        axes = [axes]\n    for i, sample_idx in enumerate(idx):\n        # Open images and masks\n        img = Image.open(image_list[sample_idx])\n        superpixel_mask = Image.open(superpixel_mask_list[sample_idx])\n        binary_mask = Image.open(binary_mask_list[sample_idx])\n        # Plot the original image\n        axes[i][0].imshow(img)\n        axes[i][0].set_title(f\"Image: {sample_idx},Size:{img.size}\")\n        axes[i][0].axis(\"off\")\n        # Plot the binary mask\n        axes[i][1].imshow(binary_mask)\n        axes[i][1].set_title(f\"Binary Mask: {sample_idx},Size:{binary_mask.size}\")\n        axes[i][1].axis(\"off\")\n        # Plot the superpixel mask\n        axes[i][2].imshow(superpixel_mask)\n        axes[i][2].set_title(f\"Superpixel Mask: {sample_idx},Size:{superpixel_mask.size}\")\n        axes[i][2].axis(\"off\")\n    # Adjust layout and display\n    plt.tight_layout()\n    plt.show()\n","metadata":{"id":"Wt-W8GxksGMS","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plot_random_images(image_list=image_path_list,\n                   binary_mask_list=binary_mask_list,\n                   superpixel_mask_list=superpixel_mask_list,\n                   num_samples=3)","metadata":{"id":"uNJnt8eUwRSB","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 2.2 - Combining Masks\n\n1. Weighted Average or Overlaying\n\n2. Union\n\n3. Intersection\n\n4. Boundary Enhancement","metadata":{"id":"wUFrpvNS6gF3"}},{"cell_type":"code","source":"import cv2\n\ndef load_masks(binary_mask_path_list, superpixel_mask_path_list,):\n  # Randomly sample indices\n  # Check for consistency in list lengths\n  if len(binary_mask_path_list) != len(superpixel_mask_path_list):\n      raise ValueError(\"The binary and superpixel mask lists must have the same length.\")\n  # Randomly select a single index\n  idx = random.choice(range(len(binary_mask_path_list)))\n  # Load masks as grayscale images\n  binary_mask = cv2.imread(binary_mask_path_list[idx], cv2.IMREAD_GRAYSCALE)\n  superpixel_mask = cv2.imread(superpixel_mask_path_list[idx], cv2.IMREAD_GRAYSCALE)\n  return binary_mask, superpixel_mask\n\nbm, sm = load_masks(\n    binary_mask_path_list=binary_mask_list,\n    superpixel_mask_path_list=superpixel_mask_list\n)","metadata":{"id":"YBQZsca38u2h","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import cv2 \nimport matplotlib.pyplot as plt\nplt.imshow(cv2.cvtColor(bm, cv2.COLOR_BGR2RGB)) \nplt.title('Image') \nplt.axis('off')","metadata":{"id":"nkn-4yYF_OK3","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.imshow(cv2.cvtColor(sm, cv2.COLOR_BGR2RGB)) \nplt.title('Image') \nplt.axis('off')","metadata":{"id":"S6Sz0pjg98KE","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 2.3 - Weighted average or Overlaying","metadata":{"id":"RHJ1tQ_o6_8Q"}},{"cell_type":"code","source":"import cv2\nimport numpy as np\nfrom pathlib import Path\n\ndef weighted_average_combination(binary_mask,superpixel_mask,alpha=0.5):\n  # Normalize masks to range [0, 1]\n  binary_mask = binary_mask.astype(np.float32) / 255.0\n  superpixel_mask = superpixel_mask.astype(np.float32) / 255.0\n  # Compute weighted average\n  combined_mask = alpha * binary_mask + (1 - alpha) * superpixel_mask\n  # Clip values to [0, 1] and scale back to [0, 255]\n  combined_mask = np.clip(combined_mask, 0, 1)\n  return (combined_mask * 255).astype(np.uint8)","metadata":{"id":"hnuz2Hr_7RYX","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"wac=weighted_average_combination(bm,sm)\nplt.imshow(cv2.cvtColor(wac, cv2.COLOR_BGR2RGB)) \nplt.title('Image') \nplt.axis('off')","metadata":{"id":"rNAdsGO9_k5h","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"lets try and overlay the orignal image with each mask and try to better understand our data","metadata":{"id":"C4p6b1NLJekV"}},{"cell_type":"markdown","source":"overlaying binary mask with orignal image","metadata":{"id":"ZVivYDe8Jy-x"}},{"cell_type":"code","source":"bm,img=load_masks(\n    binary_mask_path_list=binary_mask_list,\n    superpixel_mask_path_list=image_path_list,#insted of superpixel_mask_path_list we are send the image paths list\n)","metadata":{"id":"-CA-EeuGKGdF","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB)) \nplt.title('Image') \nplt.axis('off')","metadata":{"id":"CoSNWRzUKR8q","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"wac=weighted_average_combination(bm,img)\n\nplt.imshow(cv2.cvtColor(wac, cv2.COLOR_BGR2RGB)) \nplt.title('Image') \nplt.axis('off')","metadata":{"id":"ELyFy4q1Ktiz","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"overlaying subinaryperpixel mask with orignal image","metadata":{"id":"iLCeFbEeLYx3"}},{"cell_type":"code","source":"img,sm=load_masks(\n    binary_mask_path_list=image_path_list,#insted of binary_mask_path_list we are send the image paths list\n    superpixel_mask_path_list=superpixel_mask_list)","metadata":{"id":"cCVVmnJsLh3M","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"wac=weighted_average_combination(img,sm)\n\nplt.imshow(cv2.cvtColor(wac, cv2.COLOR_BGR2RGB)) \nplt.title('Image') \nplt.axis('off')","metadata":{"id":"VvEQrQyHMAhy","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 3 - Creating our (UNET) model class","metadata":{"id":"f9Gxh9j8UEet"}},{"cell_type":"code","source":"from collections import OrderedDict\nimport torch\nimport torch.nn as nn\n\nclass UNET (nn.Module):\n\n  def __init__(self,in_channels=3,out_channels=1,init_features=32):\n    super().__init__()\n    features=init_features\n    self.encoder1=UNET._block(in_channels,features,name=\"encoder_1\")\n    self.pool1=nn.MaxPool2d(kernel_size=2,stride=2)\n    self.encoder2=UNET._block(features,features*2,name=\"encoder_2\")\n    self.pool2=nn.MaxPool2d(kernel_size=2,stride=2)\n    self.encoder3=UNET._block(features*2,features*4,name=\"encoder_3\")\n    self.pool3=nn.MaxPool2d(kernel_size=2,stride=2)\n    self.encoder4=UNET._block(features*4,features*8,name=\"encoder_4\")\n    self.pool4=nn.MaxPool2d(kernel_size=2,stride=2)\n\n    # bottle neck\n\n    self.bottleneck=UNET._block(features*8,features*16,name=\"bottleneck\")\n\n    # Decoer\n    self.upconv4=nn.ConvTranspose2d(in_channels=features*16,out_channels=features*8,kernel_size=2,stride=2)\n    self.decoder4=UNET._block((features*8)*2,features*8,name=\"decoder_4\")\n    self.upconv3=nn.ConvTranspose2d(in_channels=features*8,out_channels=features*4,kernel_size=2,stride=2)\n    self.decoder3=UNET._block((features*4)*2,features*4,name=\"decoder_3\")\n    self.upconv2=nn.ConvTranspose2d(in_channels=features*4,out_channels=features*2,kernel_size=2,stride=2)\n    self.decoder2=UNET._block((features*2)*2,features*2,name=\"decoder_2\")\n    self.upconv1=nn.ConvTranspose2d(in_channels=features*2,out_channels=features,kernel_size=2,stride=2)\n    self.decoder1=UNET._block(features*2,features,name=\"decoder_1\")\n\n    # output\n\n    self.conv=nn.Conv2d(in_channels=features,out_channels=out_channels,kernel_size=1)\n\n  def forward(self,x):\n\n    # encoder\n    encoder_1=self.encoder1(x)\n    encoder_2=self.encoder2(self.pool1(encoder_1))\n    encoder_3=self.encoder3(self.pool2(encoder_2))\n    encoder_4=self.encoder4(self.pool3(encoder_3))\n\n    # bottleneck\n\n    bottleneck=self.bottleneck(self.pool4(encoder_4))\n\n    # decoder\n\n    decoder_4=self.upconv4(bottleneck)\n    decoder_4=torch.cat((decoder_4,encoder_4),dim=1)\n    decoder_4=self.decoder4(decoder_4)\n\n    decoder_3=self.upconv3(decoder_4)\n    decoder_3=torch.cat(tensors=(decoder_3,encoder_3),dim=1)\n    decoder_3=self.decoder3(decoder_3)\n\n    decoder_2=self.upconv2(decoder_3)\n    decoder_2=torch.cat(tensors=(decoder_2,encoder_2),dim=1)\n    decoder_2=self.decoder2(decoder_2)\n\n    decoder_1=self.upconv1(decoder_2)\n    decoder_1=torch.cat(tensors=(decoder_1,encoder_1),dim=1)\n    decoder_1=self.decoder1(decoder_1)\n\n    # final segmentation map\n    return torch.sigmoid(self.conv(decoder_1))\n\n  def _block(in_channels,features,name):\n    seq=nn.Sequential(\n        OrderedDict([\n            (name + \"conv1\",nn.Conv2d(in_channels=in_channels,out_channels=features,kernel_size=3,padding=1,bias=False)),\n            (name + \"norm1\",nn.BatchNorm2d(num_features=features)),\n            (name + \"relu1\",nn.ReLU(inplace=True)),\n            (name + \"conv2\",nn.Conv2d(in_channels=features,out_channels=features,kernel_size=3,padding=1,bias=False)),\n            (name + \"norm2\",nn.BatchNorm2d(num_features=features)),\n            (name + \"relu2\",nn.ReLU(inplace=True)),\n        ])\n    )\n    return seq","metadata":{"id":"V7poyfhLoOlb","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"💀 **Error Made**\n\n\n\n *What Happened with the Input Size of (244, 244)?*\n\n\n\n\n\n>`# creating transform\n transform=transforms.Compose([transforms.Resize(size=(244,244)),\n                                      transforms.ToTensor()])`\"\n\n                                      \n\n                                      \n\nWhen the input size was (244, 244), you encountered the error because the spatial dimension was not divisible by 16 (after 4 downsampling operations). Let's break it down:\n\n\n\n1. Initial Input: 244x244\n\n2. After 1st MaxPool: 244 / 2 = 122\n\n3. After 2nd MaxPool: 122 / 2 = 61\n\n4. After 3rd MaxPool: 61 / 2 = 30.5 (not an integer)\n\n\n\nSince the dimensions after the pooling steps should always be integers, the fact that 244 / 2^4 = 15.25 (i.e., a non-integer) causes the model to fail, especially during upsampling, which uses a ConvTranspose2d (up-conv) layer that requires the output size to match the input size during concatenation.\"\n\n\n\n\n","metadata":{"id":"xUAT4R4f5tLI"}},{"cell_type":"code","source":"# # wil result in error\n\n# model_0=UNET()\n# dummy_in=torch.rand(size=(1,3,244,244))\n# output=model_0(dummy_in)\n# print(output)","metadata":{"id":"CLWJ_jGl8yxp","colab":{"base_uri":"https://localhost:8080/","height":339},"outputId":"5de31f49-1fd5-4c7c-aaa6-e6f188bf1414","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"🟢 **Error Corrected:**\n\n\n\n*Why (224, 224) Works:*\n\n\n\n> `transform=transforms.Compose([transforms.Resize(size=(224,224)),\n  transforms.ToTensor()])`\n\n\n\nWhen you change the input size to (224, 224):\n\n\n\n1. Initial Input: 224x224\n\n2. After 1st MaxPool: 224 / 2 = 112\n\n3. After 2nd MaxPool: 112 / 2 = 56\n\n4. After 3rd MaxPool: 56 / 2 = 28\n\n5. After 4th MaxPool: 28 / 2 = 14\n\n\n\nHere, 224 is divisible by 16, meaning that after 4 downsampling operations, the spatial dimension (14x14) is compatible for the decoder's upsampling. The size of the feature maps at each stage remains consistent, and the skip connections can concatenate without issues.","metadata":{"id":"_x0sUcZm844a"}},{"cell_type":"code","source":"# will not result in error\nmodel_0=UNET()\ndummy_in=torch.rand(size=(1,3,256,256))\n\noutput=model_0(dummy_in)\n\nprint(output.shape)","metadata":{"id":"QVPLD3Yj889i","colab":{"base_uri":"https://localhost:8080/"},"outputId":"098799bb-ecce-456d-f4b3-68085940caac","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 3.1 Use `torchinfo` to get an idea of the shapes going through our model","metadata":{"id":"NcGU52LT9bCG"}},{"cell_type":"code","source":"try:\n  from torchinfo import summary\nexcept:\n  !pip install torchinfo\n  from torchinfo import summary","metadata":{"id":"GDYRvdYx-B3V","colab":{"base_uri":"https://localhost:8080/"},"outputId":"7f816aa5-043d-44c1-aa1f-f09fdd47cddc","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"summary(model=model_0,input_size=(1,3,256,256))","metadata":{"id":"sA7DRGy6-Ecw","colab":{"base_uri":"https://localhost:8080/"},"outputId":"f193efac-d75f-4927-cdd2-8fdd4a1d460c","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 4 - Custom Dataset","metadata":{"id":"Fws_45w93yi_"}},{"cell_type":"code","source":"import albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nfrom torch.utils.data import Dataset\nfrom PIL import Image\nfrom torchvision.transforms import ToTensor\nfrom torchvision import transforms\nfrom typing import Tuple\nimport numpy as np\nimport cv2\nfrom pathlib import Path\n\n\n\nclass ISICDataset(Dataset):\n  def __init__(self,image_dir_path,binarymasks_dir_path,transform=transforms.ToTensor()):\n    self.image_paths=sorted(list(Path(image_dir_path).glob(\"*.jpg\")))\n    self.binarymask_paths=sorted(list(Path(binarymasks_dir_path).glob(\"*.png\")))\n    self.transform=transform\n    assert len(self.image_paths) == len(self.binarymask_paths), (\n            \"Number of images and binary masks do not match.\"\n        )\n\n  def load_image(self,index:int)->Image.Image:\n    image_path=self.image_paths[index]\n    image=Image.open(image_path)\n    return np.array(image)\n\n  def load_binarymask(self,index:int)->Image.Image:\n    binarymask_path=self.binarymask_paths[index]\n    mask=Image.open(binarymask_path).convert(\"1\") #load mask as binary image\n    return np.array(mask, dtype=np.uint8)  # Convert to NumPy array\n\n  def __len__(self)->int:\n    return len(self.image_paths)\n\n  def __getitem__(self, index: int) -> Tuple[torch.Tensor, torch.Tensor]:\n    # Load image and mask\n    image = self.load_image(index)\n    mask = self.load_binarymask(index)\n\n    if self.transform:\n        # Apply augmentations using albumentations\n        augmented = self.transform(image=image, mask=mask)\n        image = augmented['image']\n        mask = augmented['mask'].float()\n    else:\n        # Convert to tensor if no transform is provided\n        image = ToTensor()(image)\n        mask = torch.tensor(mask, dtype=torch.float32)  # Explicitly convert to PyTorch tensor\n\n    # Ensure the mask is in shape (1, H, W) for loss compatibility\n    mask = mask.unsqueeze(0)  # Add a channel dimension\n\n    return image, mask","metadata":{"id":"V3Qb6AXsSpmw","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 5 - Dataset and Dataloaders","metadata":{"id":"TSXuzsk7ii9e"}},{"cell_type":"code","source":"# error transform\n# transform=transforms.Compose([transforms.Resize(size=(244,244)),\n#                                       transforms.ToTensor()])\n\n# creating right transform\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\n\n# Define transformation pipeline\ntransform = A.Compose([\n    A.Resize(256, 256),                        # Resize both image and mask\n    A.Rotate(limit=35, p=1.0),                 # Rotate image and mask consistently\n    A.HorizontalFlip(p=0.5),                   # Flip image and mask horizontally\n    A.VerticalFlip(p=0.1),                     # Flip image and mask vertically\n    A.Normalize(mean=(0.5,), std=(0.5,)),      # Normalize image (optional)\n    ToTensorV2()                               # Convert to PyTorch tensors\n])","metadata":{"id":"UR-z-RawnZrM","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# creating our training datasets\ntrain_dataset=ISICDataset(image_dir_path=\"ISIC-2017_Data/train/Images\",binarymasks_dir_path=\"ISIC-2017_Data/train/Binary\",transform=transform)\nimg,mask=train_dataset[0]\nprint(img.shape,mask.shape)","metadata":{"id":"ccWSvtnBblag","colab":{"base_uri":"https://localhost:8080/"},"outputId":"d8b2252a-1f42-4a6d-9350-a6fd46e91b94","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# creating our validation datasets\n\nvalid_dataset=ISICDataset(image_dir_path=\"ISIC-2017_Data/valid/Images\",binarymasks_dir_path=\"ISIC-2017_Data/valid/Binary\",transform=transform)\nimg,mask=valid_dataset[0]\nprint(\"Image shape:\", img.shape)\nprint(\"Mask shape:\", mask.shape)\nprint(\"Mask unique values:\", torch.unique(mask))","metadata":{"id":"1xUOSVRRw9_R","colab":{"base_uri":"https://localhost:8080/"},"outputId":"9e4f7fe2-d842-4818-ec82-b33560cd32ea","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Visualizing train dataset","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Assuming `sample_img` and `sample_mask` are obtained from the dataset\nsample_img, sample_mask = train_dataset[0]\n\n# Convert image and mask to numpy arrays for visualization\nimage_np = sample_img.permute(1, 2, 0).numpy()  # Change from [C, H, W] to [H, W, C]\nmask_np = sample_mask.squeeze(0).numpy()        # Remove channel dimension\n\n# Plot using subplots\nfig, axes = plt.subplots(1, 2, figsize=(12, 6))\n\n# Plot the image\naxes[0].imshow(image_np, cmap='gray')\naxes[0].set_title(\"Image\")\naxes[0].axis('off')\n\n# Plot the mask\naxes[1].imshow(mask_np, cmap='gray')\naxes[1].set_title(\"Mask\")\naxes[1].axis('off')\n\n# Show the plots\nplt.tight_layout()\nplt.show()\n","metadata":{"id":"ThqKnJkJxTBS","colab":{"base_uri":"https://localhost:8080/","height":307},"outputId":"c445f23c-2d2d-408d-edc1-7cece6a5c569","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Assuming `sample_img` and `sample_mask` are obtained from the dataset\nsample_img, sample_mask = valid_dataset[0]\n\n# Convert image and mask to numpy arrays for visualization\nimage_np = sample_img.permute(1, 2, 0).numpy()  # Change from [C, H, W] to [H, W, C]\nmask_np = sample_mask.squeeze(0).numpy()        # Remove channel dimension\n\n# Plot using subplots\nfig, axes = plt.subplots(1, 2, figsize=(12, 6))\n\n# Plot the image\naxes[0].imshow(image_np, cmap='gray')\naxes[0].set_title(\"Image\")\naxes[0].axis('off')\n\n# Plot the mask\naxes[1].imshow(mask_np, cmap='gray')\naxes[1].set_title(\"Mask\")\naxes[1].axis('off')\n\n# Show the plots\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"len(train_dataset),len(valid_dataset)","metadata":{"id":"vg7yTwkHunkL","colab":{"base_uri":"https://localhost:8080/"},"outputId":"e6686e5e-929c-46f0-fae7-aa94c05f19d8","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Creating dataloaders\n\nimport os\nfrom torch.utils.data import DataLoader\nBATCH_SIZE=8\nNUM_WORKERS=os.cpu_count()\n\ntrain_dataloader=DataLoader(dataset=train_dataset,batch_size=BATCH_SIZE,\n                            shuffle=True,num_workers=NUM_WORKERS,\n                            pin_memory=True)\n\nvalid_dataloader=DataLoader(dataset=valid_dataset,batch_size=32,\n                            shuffle=False,num_workers=NUM_WORKERS,\n                            pin_memory=True)\n\ntrain_dataloader,valid_dataloader","metadata":{"id":"sGlD7vxUkWX0","colab":{"base_uri":"https://localhost:8080/"},"outputId":"52b48a5b-4b54-45f1-bfc0-c4f79670031b","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# checking our dataloaders\nimg,mask=next(iter(train_dataloader))\nimg.shape,mask.shape","metadata":{"id":"3fFixAtSmBzM","colab":{"base_uri":"https://localhost:8080/"},"outputId":"6612be08-763f-4709-da26-c8dbcf1c6309","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# lets see if mask contain only bonary values?\nmask","metadata":{"id":"1WCorS-m01xM","colab":{"base_uri":"https://localhost:8080/"},"outputId":"e58b1118-7089-4d25-e802-accd19f94770","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 6 - Create train and test\n","metadata":{"id":"Nx0KDf74qd4-"}},{"cell_type":"markdown","source":"### 6.1 - Creating trainig and test step\n\n* `train_step()` - takes in a model and dataloader and trains the model on the dataloader.\n\n* `validation_step()` - takes in a model and dataloader and evaluates the model on the dataloader.","metadata":{"id":"gr9iZiqKOGow"}},{"cell_type":"code","source":"try:\n  import torchmetrics\nexcept:\n  !pip install torchmetrics\n  import torchmetrics","metadata":{"id":"6_3MYGw-PQTY","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torch.amp import autocast, GradScaler\n\n# Initialize a GradScaler for scaling the gradients during the backward pass\nscaler = GradScaler()\n\ndef train_step(model: nn.Module, dataloader: torch.utils.data.DataLoader,\n               loss_fn: nn.Module, optimizer: torch.optim.Optimizer,\n               metric_fn=None, device=None):\n    \"\"\"\n    Perform a training step to calculate the training loss and a chosen metric.\n\n    Args:\n        model (nn.Module): The model to train.\n        dataloader (torch.utils.data.DataLoader): The training dataloader.\n        loss_fn (nn.Module): Loss function to use.\n        optimizer (torch.optim.Optimizer): Optimizer for model updates.\n        metric_fn (callable, optional): Metric function to evaluate performance. Default is None.\n        device (torch.device, optional): Device to perform computation on. Default is None.\n\n    Returns:\n        tuple: Training loss and metric value.\n    \"\"\"\n    model.train()\n    device_type = device.type\n\n    # Initialize gradient scaler for mixed precision\n    scaler = torch.cuda.amp.GradScaler()\n\n    # Initialize metrics\n    train_loss = 0\n    train_metric = 0\n\n    # Iterate through batches\n    for img, mask in dataloader:\n        img, mask = img.to(device), mask.to(device)\n\n        # Mixed Precision Training - wrap forward pass with autocast\n        with autocast(device_type=device_type):\n            y_pred = model(img)\n            loss = loss_fn(y_pred, mask)\n\n        train_loss += loss.item()\n\n        # Calculate metric if provided\n        train_metric += metric_fn(y_pred, mask).item()\n\n        # Zero gradients before backward pass\n        optimizer.zero_grad()\n\n        # Backward pass with scaled loss\n        scaler.scale(loss).backward()\n\n        # Optimizer step\n        scaler.step(optimizer)\n        scaler.update()\n\n    # Calculate average loss and metric\n    train_loss /= len(dataloader)\n    train_metric /= len(dataloader)\n\n    return train_loss, train_metric\n\n\n\ndef validation_step(model: nn.Module, dataloader: torch.utils.data.DataLoader,\n                    loss_fn: nn.Module, metric_fn, device=None):\n    \"\"\"\n    Perform a validation step to calculate the validation loss and a chosen metric.\n\n    Args:\n        model (nn.Module): The model to evaluate.\n        dataloader (torch.utils.data.DataLoader): The validation dataloader.\n        loss_fn (nn.Module): Loss function to use.\n        metric_fn (callable): Metric function to evaluate performance.\n        device (torch.device): Device to perform computation on.\n\n    Returns:\n        tuple: Validation loss and metric value.\n    \"\"\"\n    # Set model to evaluation mode\n    model.eval()\n\n    # Initialize metrics\n    val_loss = 0\n    val_metric = 0\n\n    # Turn off gradient tracking for inference\n    with torch.no_grad():\n        for img, mask in dataloader:\n            img, mask = img.to(device), mask.to(device)\n\n            # Mixed Precision - apply autocast during inference (if using mixed precision)\n            with autocast(device.type):\n                y_pred = model(img)\n                \n                # Calculate loss\n                loss = loss_fn(y_pred, mask)\n                val_loss += loss.item()\n\n                # # Convert logits to binary predictions if needed (applying sigmoid)\n                # preds = torch.sigmoid(y_pred)  # Apply sigmoid only if needed\n                # preds = (preds > 0.5).float()  # Convert to binary\n\n                # Calculate metric (e.g., accuracy, dice score, etc.)\n                val_metric += metric_fn(y_pred, mask).item()\n\n    # Average over the batches\n    val_loss /= len(dataloader)\n    val_metric /= len(dataloader)\n\n    return val_loss, val_metric\n","metadata":{"id":"TTYGvy_d4B_t","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 6.2 - Creating `train()` to combine trainig and validation step","metadata":{"id":"lvxvJh7WN2ld"}},{"cell_type":"code","source":"from tqdm.auto import tqdm\nimport time\n\n\ndef train(model:nn.Module,train_dataloader:torch.utils.data.DataLoader,\n          test_dataloader:torch.utils.data.DataLoader,\n          optimizer:torch.optim.Optimizer,loss_fn:nn.Module,\n          metric_fn,epochs,device):\n  results={\"train_loss\":[],\n           \"train_metrics\":[],\n           \"val_loss\":[],\n           \"val_metrics\":[]}\n\n  for epoch in tqdm(range(epochs)):\n    start_time=time.time()\n    train_loss,train_metrics=train_step(model,dataloader=train_dataloader,\n                                    loss_fn=loss_fn,optimizer=optimizer,\n                                    metric_fn=metric_fn,device=device)\n\n    val_loss,val_metrics=validation_step(model,dataloader=test_dataloader,\n                                     loss_fn=loss_fn,metric_fn=metric_fn,\n                                     device=device)\n\n    end_time=time.time()\n    time_taken=end_time-start_time\n    # Print out what's happening\n    print(f\"Epoch: {epoch} | Epoch time:{time_taken} s | Train loss: {train_loss:.4f} | Train Metrics: {train_acc:.4f} | val loss: {val_loss:.4f} | val Metrics: {val_acc:.4f}\")\n\n\n    # Update results dictionary\n    results[\"train_loss\"].append(train_loss)\n    results[\"train_metrics\"].append(train_metrics)\n    results[\"val_loss\"].append(val_loss)\n    results[\"val__metrics\"].append(val_metrics)\n\n  # Return the filled results at the end of the epochs\n  return results\n","metadata":{"id":"mJFBCH18OYux","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 7 - Creating Loss and Metrics Class","metadata":{}},{"cell_type":"markdown","source":"### Dice Loss Class","metadata":{}},{"cell_type":"code","source":"class DiceLoss(torch.nn.Module):\n    def __init__(self):\n        super(DiceLoss, self).__init__()\n    \n    def forward(self, preds, targets, smooth=1e-6):\n        # preds = torch.sigmoid(preds)  # Apply sigmoid to logits\n        intersection = (preds * targets).sum()\n        dice = (2. * intersection + smooth) / (preds.sum() + targets.sum() + smooth)\n        return 1 - dice","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Dice Metrics","metadata":{}},{"cell_type":"code","source":"class CustomDiceMetric(nn.Module):\n    def __init__(self, threshold=0.5, eps=1e-6):\n        \"\"\"\n        Custom Dice metric for binary segmentation.\n        \n        Args:\n            threshold (float): Threshold to binarize predictions.\n            eps (float): Small epsilon to prevent division by zero.\n        \"\"\"\n        super(CustomDiceMetric, self).__init__()\n        self.threshold = threshold\n        self.eps = eps\n\n    def forward(self, preds, targets):\n        \"\"\"\n        Compute Dice coefficient.\n        \n        Args:\n            preds (Tensor): Model predictions (logits, before sigmoid).\n            targets (Tensor): Ground truth masks (float).\n        \n        Returns:\n            Tensor: Dice coefficient.\n        \"\"\"\n        # Apply sigmoid to predictions (logits -> probabilities)\n        # preds = torch.sigmoid(preds)\n        \n        # Binarize predictions\n        preds = (preds > self.threshold).float()\n\n        # Compute Dice coefficient\n        intersection = (preds * targets).sum(dim=(1, 2, 3))  # Sum over spatial dimensions\n        union = preds.sum(dim=(1, 2, 3)) + targets.sum(dim=(1, 2, 3))\n\n        dice = (2.0 * intersection + self.eps) / (union + self.eps)\n        return dice.mean()  # Average over the batch","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 8 - Training our model","metadata":{}},{"cell_type":"code","source":"import time\nimport mlflow\nimport mlflow.pytorch\nfrom torchmetrics import Accuracy\nimport torch\nfrom tqdm import tqdm\n\ndef setup_device_train_and_save(\n    model: nn.Module,train_dataloader: torch.utils.data.DataLoader,\n    test_dataloader: torch.utils.data.DataLoader,optimizer: torch.optim.Optimizer,\n    loss_fn: nn.Module,metric_fn,\n    epochs: int,experiment_name: str = \"Skin Lesion Segmentation\",):\n    \"\"\"\n    Integrates MLflow to track the experiment details and log training curves.\n    Calls the provided train function and logs metrics to MLflow.\n    \"\"\"\n    # Device setup\n    if torch.cuda.device_count() > 1:\n        print(f\"Multiple GPUs detected: {torch.cuda.device_count()} GPUs\")\n        model = nn.DataParallel(model)\n        device = torch.device(\"cuda:0\")\n    elif torch.cuda.is_available():\n        print(\"Using a single GPU\")\n        device = torch.device(\"cuda:0\")\n    else:\n        print(\"No GPU detected, using CPU\")\n        device = torch.device(\"cpu\")\n\n    model = model.to(device)\n    metric_fn=metric_fn.to(device)\n\n    # Training loop\n    results = train(\n        model=model,\n        train_dataloader=train_dataloader,\n        test_dataloader=test_dataloader,\n        optimizer=optimizer,\n        loss_fn=loss_fn,\n        metric_fn=metric_fn,\n        epochs=epochs,\n        device=device)\n    # saving the model\n    torch.save(model.state_dict(), f\"{experiment_name}.pth\")\n    return results\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Setting model Hyperparameters\ntorch.manual_seed(42)\ntorch.cuda.manual_seed(42)\n\nEPOCHS=1\nLEARNING_RATE=0.001\n\nmodel_1=UNET(in_channels=3,out_channels=1).to(device)\n# loss_fn=torch.nn.BCEWithLogitsLoss()\nloss_fn=DiceLoss()\noptimizer=torch.optim.Adam(model_1.parameters(),lr=LEARNING_RATE)\nmetric_fn= CustomDiceMetric(threshold=0.5)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"results_1 = setup_device_train_and_save(\n    model=model_1,\n    train_dataloader=train_dataloader,\n    test_dataloader=valid_dataloader,\n    optimizer=optimizer,\n    loss_fn=loss_fn,\n    metric_fn=metric_fn,\n    epochs=EPOCHS,\n    experiment_name=\"Skin Lesion Segmentation\",\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 9 - PLotting Loss Curves","metadata":{}},{"cell_type":"code","source":"from typing import List,Dict\ndef plot_loss_curves(results:Dict[str,List[float]]):\n    \"\"\"\n    Plots the training and validation loss and accuracy curves from the results dictionary.\n\n    Args:\n        results (Dict[str, List[float]]): A dictionary containing the following keys:\n            - 'train_loss': List of training loss values.\n            - 'val_loss': List of validation loss values.\n            - 'train_metrics': List of training accuracy values.\n            - 'val_metrics': List of validation accuracy values.\n    \"\"\"\n    # Get the loss values of the results dictionary (training and validation)\n    train_loss = results[\"train_loss\"]\n    val_loss = results[\"val_loss\"]\n\n    # Get the accuracy values of the results dictionary (training and validation)\n    train_metrics = results[\"train_metrics\"]\n    val_metrics = results[\"val_metrics\"]\n\n    # Determine the number of epochs\n    epochs = range(len(val_metrics))\n\n    # Create a figure with two subplots\n    fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(15, 7))\n\n    # Plot the training and validation loss\n    ax[0].plot(epochs, train_loss, label=\"Train Loss\")\n    ax[0].plot(epochs, val_loss, label=\"Validation Loss\")\n    ax[0].set_title(\"Loss\")\n    ax[0].set_xlabel(\"Epochs\")\n    ax[0].set_ylabel(\"Loss\")\n    ax[0].legend()\n\n    # Plot the training and validation accuracy\n    ax[1].plot(epochs, train_metrics, label=\"Train Accuracy\")\n    ax[1].plot(epochs, val_metrics, label=\"Validation Accuracy\")\n    ax[1].set_title(\"Accuracy\")\n    ax[1].set_xlabel(\"Epochs\")\n    ax[1].set_ylabel(\"Accuracy\")\n    ax[1].legend()\n\n    # Display the plots\n    plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# plotting the results for out model\nplot_loss_curves(results=results_1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 7 - Model training and Experiment tracking with MLFLOW","metadata":{"id":"MrxsB0elXWqP"}},{"cell_type":"code","source":"try:\n    import mlflow\n    import mlflow.pytorch\nexcept:\n    !pip install mlflow\n    import mlflow\n    import mlflow.pytorch","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import time\nimport mlflow\nimport mlflow.pytorch\nfrom torchmetrics import Accuracy\nimport torch\nfrom tqdm import tqdm\n\ndef setup_device_and_train_with_mlflow(\n    model: nn.Module,train_dataloader: torch.utils.data.DataLoader,\n    test_dataloader: torch.utils.data.DataLoader,optimizer: torch.optim.Optimizer,\n    loss_fn: nn.Module,metric_fn,\n    epochs: int,experiment_name: str = \"Skin Lesion Segmentation\",):\n    \"\"\"\n    Integrates MLflow to track the experiment details and log training curves.\n    Calls the provided train function and logs metrics to MLflow.\n    \"\"\"\n    # Initialize MLflow experiment\n    mlflow.set_experiment(experiment_name)\n\n    with mlflow.start_run():  # Start an MLflow run\n        # Log model hyperparameters\n        mlflow.log_param(\"model_name\", model.__class__.__name__)\n        mlflow.log_param(\"epochs\", epochs)\n        mlflow.log_param(\"optimizer\", optimizer.__class__.__name__)\n        mlflow.log_param(\"loss_fn\", loss_fn.__class__.__name__)\n\n        # Device setup\n        if torch.cuda.device_count() > 1:\n            print(f\"Multiple GPUs detected: {torch.cuda.device_count()} GPUs\")\n            model = nn.DataParallel(model)\n            device = torch.device(\"cuda:0\")\n        elif torch.cuda.is_available():\n            print(\"Using a single GPU\")\n            device = torch.device(\"cuda:0\")\n        else:\n            print(\"No GPU detected, using CPU\")\n            device = torch.device(\"cpu\")\n\n        model = model.to(device)\n        metric_fn=metric_fn.to(device)\n\n        # Training loop\n        results = train(\n            model=model,\n            train_dataloader=train_dataloader,\n            test_dataloader=test_dataloader,\n            optimizer=optimizer,\n            loss_fn=loss_fn,\n            metric_fn=metric_fn,\n            epochs=epochs,\n            device=device\n        )\n        try:\n\n            # Log metrics for each epoch\n            for epoch in range(epochs):\n                # Log train and validation metrics for each epoch\n                mlflow.log_metric(\"train_loss\", results[\"train_loss\"][epoch], step=epoch)\n                mlflow.log_metric(\"train_acc\", results[\"train_acc\"][epoch], step=epoch)\n                mlflow.log_metric(\"val_loss\", results[\"val_loss\"][epoch], step=epoch)\n                mlflow.log_metric(\"val_acc\", results[\"val_acc\"][epoch], step=epoch)\n    \n            # Log final metrics\n            mlflow.log_metric(\"final_train_loss\", results[\"train_loss\"][-1])\n            mlflow.log_metric(\"final_train_acc\", results[\"train_acc\"][-1])\n            mlflow.log_metric(\"final_val_loss\", results[\"val_loss\"][-1])\n            mlflow.log_metric(\"final_val_acc\", results[\"val_acc\"][-1])\n    \n            # Log the model\n            example_input = torch.randn(1, 3, 256, 256)  # Example input matching model's input\n            example_input_np = example_input.cpu().numpy()  # Convert to numpy array\n            \n            # Now log the model with the converted input example\n            mlflow.pytorch.log_model(model, \"model\", input_example=example_input_np)\n    \n            print(\"MLflow run completed. Results are logged.\")\n        except Exception as e:\n            print(f\"An error occurred: {e}\") \n\n    return results\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Setting model parameter\n# Setting model Hyperparameters\ntorch.manual_seed(42)\ntorch.cuda.manual_seed(42)\n\nEPOCHS=1\nLEARNING_RATE=0.001\n\nmodel_2=UNET(in_channels=3,out_channels=1).to(device)\n# loss_fn=torch.nn.BCEWithLogitsLoss()\nloss_fn=DiceLoss()\noptimizer=torch.optim.Adam(model_2.parameters(),lr=LEARNING_RATE)\nmetric_fn= CustomDiceMetric(threshold=0.5)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"results_2 = setup_device_and_train_with_mlflow(\n    model=model_2,\n    train_dataloader=train_dataloader,\n    test_dataloader=valid_dataloader,\n    optimizer=optimizer,\n    loss_fn=loss_fn,\n    metric_fn=metric_fn,\n    epochs=EPOCHS,\n    experiment_name=\"Skin Lesion Segmentation\",\n)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Saving Model","metadata":{}},{"cell_type":"code","source":"# # Save the model to a file\n# torch.save(model_0.state_dict(), f\"unet_model_{EPOCHS}.pth\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plot_loss_curves(results_1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Compressing our MLflow data for downloading","metadata":{}},{"cell_type":"code","source":"def compress_folder_to_zip(folder_path: str, destination_folder: str=None, zip_filename: str=None):\n    \"\"\"\n    Compresses a folder into a zip file and saves it in the destination folder.\n\n    Args:\n        folder_path (str): The path to the folder to compress.\n        destination_folder (str): The path to the destination folder where the zip file will be saved.\n        zip_filename (str): The name of the zip file (without extension).\n    \"\"\"\n    if destination_folder == None and zip_filename==None:\n        zip_filename=os.path.basename(folder_path)\n        destination_folder=folder_path-zip_filename\n    \n    \n    # Ensure the destination folder exists\n    if not os.path.exists(destination_folder):\n        os.makedirs(destination_folder)\n\n    # Define the full path for the zip file\n    zip_file_path = os.path.join(destination_folder, zip_filename + '.zip')\n\n    # Compress the folder into a zip file\n    shutil.make_archive(zip_file_path[:-4], 'zip', folder_path)\n    \n    print(f\"Folder '{folder_path}' has been compressed and saved to '{zip_file_path}'.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import wandb\n# import torch\n# import torch.nn as nn\n# from torchmetrics import Accuracy\n\n# def setup_device_and_train_with_wandb(\n#     model: nn.Module,\n#     train_dataloader: torch.utils.data.DataLoader,\n#     valid_dataloader: torch.utils.data.DataLoader,\n#     optimizer: torch.optim.Optimizer,\n#     loss_fn: nn.Module,\n#     acc_fn: Accuracy,\n#     epochs: int,\n#     experiment_name: str = \"Skin Lesion Segmentation\",\n# ):\n#     \"\"\"\n#     Integrates Weights & Biases (W&B) to track the experiment details.\n#     \"\"\"\n#     # Initialize W&B experiment\n#     wandb.init(project=experiment_name)  # Initialize W&B run\n\n#     # Log hyperparameters\n#     wandb.config.model_name = model.__class__.__name__\n#     wandb.config.epochs = epochs\n#     wandb.config.optimizer = optimizer.__class__.__name__\n#     wandb.config.loss_fn = loss_fn.__class__.__name__\n\n#     # Device setup\n#     if torch.cuda.device_count() > 1:\n#         print(f\"Multiple GPUs detected: {torch.cuda.device_count()} GPUs\")\n#         model = nn.DataParallel(model)\n#         device = torch.device(\"cuda:0\")\n#     elif torch.cuda.is_available():\n#         print(\"Using a single GPU\")\n#         device = torch.device(\"cuda:0\")\n#     else:\n#         print(\"No GPU detected, using CPU\")\n#         device = torch.device(\"cpu\")\n\n#     model = model.to(device)\n\n#     # Watch the model and log gradients/parameters\n#     wandb.watch(model, log=\"all\")\n\n#     # Training loop\n#     results = train(\n#         model=model,\n#         train_dataloader=train_dataloader,\n#         test_dataloader=test_dataloader,\n#         optimizer=optimizer,\n#         loss_fn=loss_fn,\n#         acc_fn=acc_fn,\n#         epochs=epochs,\n#         device=device,\n#     )\n\n#     # Log metrics after training\n#     for epoch in range(epochs):\n#         wandb.log({\n#             \"train_loss\": results[\"train_loss\"][epoch],\n#             \"train_acc\": results[\"train_acc\"][epoch],\n#             \"val_loss\": results[\"val_loss\"][epoch],\n#             \"val_acc\": results[\"val_acc\"][epoch],\n#             \"epoch\": epoch,\n#         })\n\n#     # Log final metrics\n#     wandb.log({\n#         \"final_train_loss\": results[\"train_loss\"][-1],\n#         \"final_train_acc\": results[\"train_acc\"][-1],\n#         \"final_val_loss\": results[\"val_loss\"][-1],\n#         \"final_val_acc\": results[\"val_acc\"][-1]\n#     })\n\n#     # Save model to W&B\n#     wandb.save(\"model.pt\")\n\n#     print(\"W&B run completed. Results are logged.\")\n\n#     return results\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# !wandb login","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# setup_device_and_train_with_wandb(model=model_0,train_dataloader=train_dataloader,\n#                                  valid_dataloader=valid_dataloader,optimizer=optimizer,\n#                                  loss_fn=loss_fn,acc_fn=acc_fn,epochs=EPOCHS)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}